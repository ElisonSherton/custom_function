{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "code_folding": [
     29,
     42,
     55
    ]
   },
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "def create_model(model_name, modeltype):\n",
    "    \"\"\"\n",
    "    create regression\n",
    "    modeltype:\n",
    "        \"lr\"`'LinearRegression'\n",
    "        \"ridge\"`'Ridge'\n",
    "        \"kmeans\"`clustering\n",
    "        \"ann\"`'artifical neural network'\n",
    "        \n",
    "    \"\"\"\n",
    "    step1 = f\"# model import for {modeltype}\"\n",
    "    step2 = f\"# model initalize for {modeltype}\"\n",
    "    step3 = f\"# tunable parameters for {modeltype}\"\n",
    "    step4 = f\"# selecting the optimal parameter for {modeltype} \"\n",
    "    step5 = f\"# fit {modeltype} on training data\" \n",
    "    step6 = f\"# predict using {modeltype} on test data\"\n",
    "    step6 = f\"# Evaluate {modeltype}: {model_name}\"\n",
    "\n",
    "    if modeltype == \"lr\":\n",
    "        print(step1)\n",
    "        model_import = \"from sklearn.linear_model import LinearRegression\"\n",
    "        print(model_import + \"\\n\")\n",
    "\n",
    "        print(step2)\n",
    "        model_initalise = f\"{model_name} = LinearRegression()\"\n",
    "        print(model_initalise + \"\\n\")\n",
    "\n",
    "        print(step3)\n",
    "        model_tune = \"None\"\n",
    "        print(model_tune + \"\\n\")\n",
    "\n",
    "    if modeltype == \"ridge\":\n",
    "        print(step1)\n",
    "        model_import = \"from sklearn import linear_model\"\n",
    "        print(model_import + \"\\n\")\n",
    "        \n",
    "        print(step2)\n",
    "        model_initalise = f\"{model_name} = linear_model.Ridge(alpha=.5)\"\n",
    "        print(model_initalise + \"\\n\")\n",
    "        \n",
    "        print(step3)\n",
    "        model_tune = \"alpha\"\n",
    "        print(model_tune + \"\\n\")\n",
    "\n",
    "    if modeltype == \"kmeans\":\n",
    "        print(step1)\n",
    "        model_import = \"from sklearn.cluster import KMeans\"\n",
    "        print(model_import + \"\\n\")\n",
    "        \n",
    "        print(step2)\n",
    "        model_initalise = f\"{model_name} = KMeans(n_clusters=2, random_state=42)\"\n",
    "        print(model_initalise + \"\\n\")\n",
    "        \n",
    "        print(step3)\n",
    "        model_tune = \"n_clusters\"\n",
    "        print(model_tune + \"\\n\")\n",
    "\n",
    "    if modeltype == \"ann\":\n",
    "        variables = input(\"number of dimensions: \")\n",
    "        \n",
    "        print(step1)\n",
    "        model_import_1 = \"from keras.models import Sequential\"\n",
    "        model_import_2 = \"from keras.layers import Dense, Activation\"\n",
    "        print(model_import_1)\n",
    "        print(model_import_2)\n",
    "        \n",
    "         \n",
    "        step2 = f\"# model initalize for {modeltype}\"\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        step3 = f\"# tunable parameters for {modeltype}\"\n",
    "        step4 = f\"# selecting the optimal parameter for {modeltype} \"\n",
    "        step5 = f\"# fit {modeltype} on training data\" \n",
    "        step6 = f\"# predict using {modeltype} on test data\"\n",
    "        step6 = f\"# Evaluate {modeltype}: {model_name}\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_model(\"nn\",\"ann\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_task(task):\n",
    "    \"\"\"\n",
    "    task:\n",
    "    `train_test_split\n",
    "    `standardize\n",
    "    `dtypes_change\n",
    "    `one-hot\n",
    "    `\n",
    "    \"\"\"\n",
    "    step1 = f\"# module import  for {task}\"\n",
    "    step2 = f\"# module initalize for {task}\"\n",
    "    step3 = f\"# check for\"\n",
    "\n",
    "    if task == \"train_test_split\":\n",
    "        test_size = input(\"test_size: \") + \" \\n\"\n",
    "        stratify = input(\"stratify = None , y: \")\n",
    "        \n",
    "        print(step1 + f\" with test size = {test_size}\")\n",
    "        module_import= \"from sklearn.model_selection import train_test_split\"\n",
    "        print(module_import + \"\\n\")\n",
    "        \n",
    "        print(\"# Dataframe of Independent Variables \")\n",
    "        print(\"X =... \" + \"\\n\")\n",
    "        \n",
    "        print(\"# Data of Target Variable\")\n",
    "        print(\"y =...\" + \"\\n\")\n",
    "        \n",
    "        print(\"# Splitting the dataset into the Training set and Test set\")\n",
    "        model_initalise = f\"X_train, X_test, y_train,y_test = train_test_split(X,y,test_size={test_size},stratify={stratify}, random_state = 42)\"\n",
    "        print(model_initalise)\n",
    "        \n",
    "        print(\"print('train_test_split_shape: ','X_train:',X_train.shape,'y_train:', y_train.shape,'X_test:', X_test.shape,'y_test:', y_test.shape)\")\n",
    "            \n",
    "    if task == \"standardize\":\n",
    "        scaler = input(\"create scaler as:\")\n",
    "        data = input(\"create numeric_dataframe as:\")\n",
    "        scaled_data = input(\"create scaled_data as:\")\n",
    "        \n",
    "        print (step1)\n",
    "        module_import = \"from sklearn.preprocessing import StandardScaler\" \n",
    "        print (module_import + \"\\n\")\n",
    "        \n",
    "        print (step2)\n",
    "        model_initalise = f\"{scaler} = StandardScaler()\"\n",
    "        print (model_initalise + \"\\n\")\n",
    "        \n",
    "        step3 = f\"# Create object of numeric data to fit {scaler}\" \n",
    "        print (step3)\n",
    "        print (f\"{data} = ...\\n\")\n",
    "        \n",
    "        step4 = f\"# fit {scaler} object on {data}\" \n",
    "        print(step4)\n",
    "        print (f\"{scaler}.fit({data})\\n\")\n",
    "        \n",
    "        step5 = f\"# transform {data} using {scaler} and save to object {scaled_data}\"\n",
    "        print(step5)\n",
    "        print(f\"{scaled_data} = {scaler}.transform({data})\")\n",
    "        \n",
    "        \n",
    "            \n",
    "    if task == \"dtypes_change\":\n",
    "        data = input(\"dataframe name: \")\n",
    "        col_list = input(\"column names:\")\n",
    "        dtype = input(\"'object'/'int64'/'float64'/'bool'/'datetime64'/'timedelta[ns]'/'category'\")\n",
    "        print (f\"for col in[{col_list}]\",\":\")\n",
    "        print (f\"\\t{data}[col] = {data}[col].astype({dtype})\")\n",
    "        print (f\"assert {data}.loc[:,[{col_list}]].dtypes.values.all() == {dtype}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create scaler as:scaler\n",
      "create numeric_dataframe as:numeric_data\n",
      "create scaled_data as:scaled_data\n",
      "# module import  for standardize\n",
      "from sklearn.preprocessing import StandardScaler\n",
      "\n",
      "# module initalize for standardize\n",
      "scaler = StandardScaler()\n",
      "\n",
      "# Create object of numeric data to fit scaler\n",
      "numeric_data = ...\n",
      "\n",
      "# fit scaler object on numeric_data\n",
      "scaler.fit(numeric_data)\n",
      "\n",
      "# transform numeric_data using scaler and save to object scaled_data\n",
      "scaled_data = scaler.transform(numeric_data)\n"
     ]
    }
   ],
   "source": [
    "preprocess_task(\"standardize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module import  for train_test_split with test size = 0.2 \n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataframe of Independent Variables \n",
    "X = \n",
    "\n",
    "# Data of Target Variable\n",
    "y = \n",
    "\n",
    "# Splitting the dataset into the Training set and Test set\n",
    "X_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.2,stratify=y, random_state = 42)\n",
    "print('train_test_split_shape: ','X_train:',X_train.shape,'y_train:', y_train.shape,'X_test:', X_test.shape,'y_test:', y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
